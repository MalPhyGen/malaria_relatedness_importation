{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2dc1b15-fdd0-4f9b-b679-e7cf371bcefa",
   "metadata": {},
   "source": [
    "# Pre-processing data for Dcifer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6413d29-51ac-4d98-91fc-5b1736802c12",
   "metadata": {},
   "source": [
    "This notebook merges all the allele tables from all GenMoz 2022 samples with some of the metadata to be run together with Dcifer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72633210-2cf0-45d0-850c-4104e9b209f7",
   "metadata": {},
   "source": [
    "We first import the modules required for the script: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e7145da-6b05-4bba-9208-7895022caafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from genmoz_pytools.notebook_pytools import sampleID2nida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c3535d-5fa7-4dde-8fe7-606ae4cdf27e",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31385926-e0b4-490d-914e-2b7e09733829",
   "metadata": {},
   "source": [
    "We load all the allele tables from the different runs. The directories and names of the files should be modified to the specific paths and names of the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24135eea-3cec-413f-be65-a7dde416bac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#HFS data\n",
    "hfs_path = \"/home/apujol/isglobal/projects/genmoz/data/HFS/HFS2022_DB_and_pipeline_outputs/\"\n",
    "hfs_run_dir = 'Pipeline v0.1.8/'\n",
    "BOH01_filename = hfs_path + hfs_run_dir + \"allele_data_global_max_0_filtered_BOH01.csv\"\n",
    "SMC201_filename = hfs_path + hfs_run_dir + \"allele_data_global_max_0_filtered_SMC201.csv\"\n",
    "HFS2201_filename = hfs_path + hfs_run_dir + \"allele_data_global_max_0_filtered_HFS2201.csv\"\n",
    "SMC202_filename = hfs_path + hfs_run_dir + \"allele_data_global_max_0_filtered_SMC202.csv\"\n",
    "HFS2202_filename = hfs_path + hfs_run_dir + \"allele_data_global_max_0_filtered_HFS2202.csv\"\n",
    "SMC203_filename = hfs_path + hfs_run_dir + \"allele_data_global_max_0_filtered_SMC203.csv\"\n",
    "HFS2203_filename = hfs_path + hfs_run_dir + \"allele_data_global_max_0_filtered_HFS2203.csv\"\n",
    "SMC204_filename = hfs_path + hfs_run_dir + \"allele_data_global_max_0_filtered_SMC204.csv\"\n",
    "ICAB01_filename = hfs_path + hfs_run_dir + \"allele_data_global_max_0_filtered_ICAB01.csv\"\n",
    "TES2201_filename = hfs_path + hfs_run_dir + \"allele_data_global_max_0_filtered_TES2201.csv\"\n",
    "\n",
    "BOH01_data = pd.read_csv(BOH01_filename)\n",
    "BOH01_data['run'] = 'BOH01'\n",
    "SMC201_data = pd.read_csv(SMC201_filename)\n",
    "SMC201_data['run'] = 'SMC201'\n",
    "#Renaming sampleID to include 'N' initially\n",
    "for i in SMC201_data.index:\n",
    "    SMC201_data.loc[i, 'Unnamed: 0'] = 'N' + SMC201_data.loc[i, 'Unnamed: 0']\n",
    "HFS2201_data = pd.read_csv(HFS2201_filename)\n",
    "HFS2201_data['run'] = 'HFS2201'\n",
    "SMC202_data = pd.read_csv(SMC202_filename)\n",
    "SMC202_data['run'] = 'SMC202'\n",
    "HFS2202_data = pd.read_csv(HFS2202_filename)\n",
    "HFS2202_data['run'] = 'HFS2202'\n",
    "SMC203_data = pd.read_csv(SMC203_filename)\n",
    "SMC203_data['run'] = 'SMC203'\n",
    "HFS2203_data = pd.read_csv(HFS2203_filename)\n",
    "HFS2203_data['run'] = 'HFS2203'\n",
    "SMC204_data = pd.read_csv(SMC204_filename)\n",
    "SMC204_data['run'] = 'SMC204'\n",
    "ICAB01_data = pd.read_csv(ICAB01_filename)\n",
    "ICAB01_data['run'] = 'ICAB01'\n",
    "TES2201_data = pd.read_csv(TES2201_filename)\n",
    "TES2201_data['run'] = 'TES2201'\n",
    "\n",
    "#Massinga data\n",
    "massinga_path = \"/home/apujol/isglobal/projects/genmoz/data/HFS/Inhambane_Massinga_2022_MULTIPLY/\"\n",
    "massinga_dir = \"MULB_pipeline0.1.8/\"\n",
    "massinga_filename = massinga_path + massinga_dir + \"allele_data_global_max_0_filtered.csv\"\n",
    "massinga_data = pd.read_csv(massinga_filename)\n",
    "massinga_data['run'] = 'MULT'\n",
    "\n",
    "#REACT2 data\n",
    "react_path = \"/home/apujol/isglobal/projects/genmoz/data/Pipeline_Results/\"\n",
    "react2_dir1 = \"REACT2_NextSEQ01_140623_modif_dup_RESULTS_v0.1.8/\"\n",
    "react2_dir2 = \"REACT2_NextSeq02_RESULTS_v0.1.8/\"\n",
    "boh22_dir = \"BOH22_Nextseq02_RESULTS_v0.1.8_FILTERED/\"\n",
    "\n",
    "react2_run1_filename = react_path + react2_dir1 + \"allele_data_global_max_0_filtered.csv\"\n",
    "react2_run2_filename = react_path + react2_dir2 + \"allele_data_global_max_0_filtered.csv\"\n",
    "boh22_run_filename = react_path + boh22_dir + \"allele_data_global_max_0_filtered.csv\"\n",
    "\n",
    "react2_run1_data = pd.read_csv(react2_run1_filename)\n",
    "react2_run1_data['run'] = 'REACT_R1'\n",
    "react2_run2_data = pd.read_csv(react2_run2_filename)\n",
    "react2_run2_data['run'] = 'REACT_R2'\n",
    "boh22_run_data = pd.read_csv(boh22_run_filename)\n",
    "boh22_run_data['run'] = 'BOH22'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68344d45-3f2f-42ed-9c1e-06c47350bba6",
   "metadata": {},
   "source": [
    "We join all the data from the HFS+TES data to the same table called hfs_run_data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de27037e-70d3-4c74-ad3d-61426f7947a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Concatenating all data from HFS\n",
    "hfs_run_data = pd.concat([BOH01_data, SMC201_data, HFS2201_data, SMC202_data, HFS2202_data, SMC203_data, HFS2203_data, SMC204_data, ICAB01_data, TES2201_data], ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eefa4b-853a-4c3b-9f0f-7d4a7234f41e",
   "metadata": {},
   "source": [
    "## Adding nida numbers to all data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d820c160-8d47-4d88-8157-0926ff2a0e90",
   "metadata": {},
   "source": [
    "Some data files do not have a nida number (a numeric identifier of the sample), but it can be derived from the variable sampleID. In the next slides, we fix the name of one sample that was wrong (sampleID = N19401519_2_S218), and we generate the variable nida for all the samples of our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da3dd616-1919-467d-95bd-77f38d1460b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing wrong nida number\n",
    "wrong_nida = react2_run2_data['sampleID'] == 'N19401519_2_S218'\n",
    "react2_run2_data.loc[wrong_nida, 'sampleID'] = 'N1941519_2_S218'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c197f37-6bd6-408c-9d4e-0bcc729216b6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HFS done\n",
      "Massinga done\n",
      "REACT_R1 done\n",
      "REACT_R2 done\n",
      "BOH22 done\n"
     ]
    }
   ],
   "source": [
    "hfs_run_data = hfs_run_data.rename(columns = {'Unnamed: 0' : 'sampleID'})\n",
    "hfs_run_data = sampleID2nida(hfs_run_data)\n",
    "print('HFS' + ' done')\n",
    "massinga_data = massinga_data.rename(columns = {'Unnamed: 0' : 'sampleID'})\n",
    "massinga_data = sampleID2nida(massinga_data)\n",
    "print('Massinga' + ' done')\n",
    "react2_run1_data = sampleID2nida(react2_run1_data)\n",
    "print('REACT_R1' + ' done')\n",
    "react2_run2_data = sampleID2nida(react2_run2_data)\n",
    "print('REACT_R2' + ' done')\n",
    "boh22_run_data = sampleID2nida(boh22_run_data) \n",
    "print('BOH22' + ' done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7056f22-ab17-4822-b0f5-9c3570535297",
   "metadata": {},
   "source": [
    "## Removing wrong runs from REACT2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd4da48-ef2d-4474-8ab0-95af5ae08f57",
   "metadata": {},
   "source": [
    "The first run of 19 samples from REACT2 (the project involved in the data and sample collection from Magude and Matutuine districts) were wrong and need to be excluded from the analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f425e34-b690-4638-9800-b7efa20ccfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a list of samples from REACT2 that are believed to have given wrong output results. \n",
    "#They were repeated in BOH22 run, and they should be excluded from the first two runs\n",
    "list_react2_nida_to_exclude = [1974289.2, 1974249.6, 1941507.9, 1974170.3, \\\n",
    "                       1974006.5, 1974025.6, 1974016.4, 1941589.5, \\\n",
    "                       1941487.4, 1928588.7, 1974086.7, 1974532.9, \\\n",
    "                       1974063.8, 2038793.8, 2039543.8, 1941699.1, \\\n",
    "                        1990098.8, 2084295.6, 2084332.8]\n",
    "\n",
    "react1_mask = react2_run1_data['nida'].notnull()\n",
    "for nida in list_react2_nida_to_exclude:\n",
    "    react1_mask = react1_mask&(react2_run1_data['nida'] != nida)\n",
    "\n",
    "react2_run1_data = react2_run1_data[react1_mask]\n",
    "\n",
    "react2_mask = react2_run2_data['nida'].notnull()\n",
    "for nida in list_react2_nida_to_exclude:\n",
    "    react2_mask = react2_mask&(react2_run2_data['nida'] != nida)\n",
    "\n",
    "react2_run2_data = react2_run2_data[react2_mask]\n",
    "\n",
    "#Testing that the nidas have been successfully removed from REACT runs 1 and 2, it should give no output text\n",
    "for data in [react2_run1_data, react2_run2_data]:\n",
    "    final_nidas = data['nida'].unique()\n",
    "    for nida in final_nidas: \n",
    "        if nida in list_react2_nida_to_exclude:\n",
    "            print(nida)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b8cd60-7d8b-4f9b-a887-18c9dd4aa5c8",
   "metadata": {},
   "source": [
    "## Import HFS and Massinga metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6575639c-2911-4565-a7e8-a21ad6744214",
   "metadata": {},
   "source": [
    "Here we import the variables of study of the data from the different provinces of Mozambique (all samples except those from Matutuine and Magude districts in Maputo province). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f65c2b30-d3ab-479b-86e7-b8256a9e25a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_95200/2028594987.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  hfs_meta['source'] = 'HFS'\n",
      "/tmp/ipykernel_95200/2028594987.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  massinga_meta['source'] = 'Multiply'\n",
      "/tmp/ipykernel_95200/2028594987.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  massinga_meta['region'] = 'South'\n"
     ]
    }
   ],
   "source": [
    "#HFS metadata file name\n",
    "hfs_meta_filename = hfs_path + \"HFS2022_random_sequenced_rainy_DB220624.dta\"\n",
    "#Load HFS metadata\n",
    "hfs_meta = pd.read_stata(hfs_meta_filename)\n",
    "#Create variable called 'source'\n",
    "hfs_meta['source'] = 'HFS'\n",
    "#Define columns to include\n",
    "hfs_meta_columns = ['nida', 'study_nr', 'study', 'year', 'region', \\\n",
    "                    'province', 'district', 'post_admin', 'us', 'age', \\\n",
    "                    'sex', 'study_lab', 'parasitemia', \\\n",
    "                    'run_id_resmark', 'run_id', 'source']\n",
    "\n",
    "#Massinga (Inhambane province) metadata file name\n",
    "massinga_meta_filename = massinga_path + \"InhambaneMassinga_2022_MULTIPLY_resmarkers_DB.dta\"\n",
    "#Load metadata from Massinga \n",
    "massinga_meta = pd.read_stata(massinga_meta_filename)\n",
    "#Specify source and region variables\n",
    "massinga_meta['source'] = 'Multiply'\n",
    "massinga_meta['region'] = 'South'\n",
    "#Rename columns for consistency with HFS data\n",
    "massinga_meta = massinga_meta.rename(columns = {'provincefactor' : 'province', \\\n",
    "                                               'districtfactor' : 'district', \\\n",
    "                                               'usfactor' : 'us', \\\n",
    "                                               'sexfactor' : 'sex', \\\n",
    "                                               'STUDY' : 'study', \\\n",
    "                                               'RUN' : 'run_id'})\n",
    "#Define columns to include\n",
    "massinga_meta_columns = ['nida', 'study_nr', 'post_admin', 'age', \\\n",
    "                         'province', 'district', 'us', \\\n",
    "                         'sex', 'study','run_id', 'source', 'region']\n",
    "\n",
    "#Converting nida to float\n",
    "massinga_meta.loc[massinga_meta['nida'] == '', 'nida'] = np.nan\n",
    "massinga_meta['nida'] = pd.Series(massinga_meta['nida'], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd96c5d-2df4-4e05-94bd-c85e4aa40300",
   "metadata": {},
   "source": [
    "## Merge HFS data and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9391022f-d5fc-4e75-8e7f-93d2331cfe33",
   "metadata": {},
   "source": [
    "Here we merge the metadata from HFS data (only the selected columns) with their allele table data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29108fc9-822e-4d80-a172-d6e25ba10251",
   "metadata": {},
   "outputs": [],
   "source": [
    "hfs_data_merged = pd.merge(hfs_run_data, hfs_meta[hfs_meta_columns], on = 'nida', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3462e19d-e7a8-4c27-b7e0-8b84ba482ad9",
   "metadata": {},
   "source": [
    "We want to avoid using duplicate samples in the analysis. For this, we look for samples (nida) that are duplicated. When found, the one with highest read counts is selected for the study. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec9978dd-6d57-4843-9613-9da05172cbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated NIDA: [1939665.1 1939610.1 1939667.5]\n"
     ]
    }
   ],
   "source": [
    "#Identify duplicated nidas\n",
    "mask = hfs_data_merged[['allele', 'nida']].duplicated()\n",
    "duplicated_nidas = hfs_data_merged.loc[mask, 'nida'].unique()\n",
    "print(\"Duplicated NIDA:\", duplicated_nidas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddfc0200-b6f1-4c41-ba93-07b128f2b537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated nida: 1939665.1\n",
      "N1939665_1_S67 152356.0\n",
      "N1939665_1_S2 218917.0\n",
      "kept sampleID: N1939665_1_S2\n",
      "\n",
      "Duplicated nida: 1939610.1\n",
      "N1939610_1_S4 12643.0\n",
      "N1939610_1_S98 486114.0\n",
      "kept sampleID: N1939610_1_S98\n",
      "\n",
      "Duplicated nida: 1939667.5\n",
      "N1939667_5_S198 1245575.0\n",
      "N1939667_5_S1 333899.0\n",
      "kept sampleID: N1939667_5_S198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exploring the read counts for the duplicates and select the one with highest depth\n",
    "for nida in duplicated_nidas: \n",
    "    print(\"Duplicated nida:\", nida)\n",
    "    #Identify samples with the same nida\n",
    "    sample_list = hfs_data_merged.loc[hfs_data_merged['nida'] == nida, 'sampleID'].unique()\n",
    "    #Sum all read counts across all alleles for each sample\n",
    "    all_counts = np.zeros(len(sample_list))\n",
    "    for s, sample in enumerate(sample_list):\n",
    "        mask_sample = hfs_data_merged['sampleID'] == sample\n",
    "        all_counts[s] = hfs_data_merged.loc[mask_sample, ['sampleID', 'reads']].sum().iloc[1]\n",
    "        print(sample, all_counts[s])\n",
    "    #Identify the sample with highest depth\n",
    "    where_max_reads = np.where(all_counts == np.max(all_counts))[0][0]\n",
    "    kept_sample = sample_list[where_max_reads]\n",
    "    print(\"kept sampleID:\", kept_sample)\n",
    "    #filtering samples\n",
    "    for s, sample in enumerate(sample_list):\n",
    "        if sample != kept_sample:\n",
    "            hfs_data_merged = hfs_data_merged.loc[hfs_data_merged['sampleID'] != sample]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf819c9-d5bf-45fe-a10d-5b4edafb2765",
   "metadata": {},
   "source": [
    "## Merge Massinga data and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a53a39b-8679-418c-85b3-c37e3b3d1dc5",
   "metadata": {},
   "source": [
    "Now we proceed to merge the allele table data with the metadata (the selected columns only) for the samples from Massinga:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "852b6de2-f78b-41dd-8440-956c667bb96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "massinga_data_merged = pd.merge(massinga_data, massinga_meta.loc[massinga_meta['nida'].notnull(), massinga_meta_columns], on = 'nida', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48422616-0900-484d-a139-857801530e2e",
   "metadata": {},
   "source": [
    "We repeat the same procedure to identify duplicated nida and select the run with higher read counts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07a6c067-c396-433e-98f2-d10aef2c2a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated NIDA: [1975164.1 1975177.1 1975194.8 1975247.1]\n"
     ]
    }
   ],
   "source": [
    "#Identify duplicated nidas\n",
    "mask = massinga_data_merged[['allele', 'nida']].duplicated()\n",
    "duplicated_nidas = massinga_data_merged.loc[mask, 'nida'].unique()\n",
    "print(\"Duplicated NIDA:\", duplicated_nidas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0301183-e5a2-434d-ba66-d2ee7d005d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated nida: 1975164.1\n",
      "N1975164_1a_S210 214955.0\n",
      "N1975164_1b_S259 122519.0\n",
      "kept sampleID: N1975164_1a_S210\n",
      "\n",
      "Duplicated nida: 1975177.1\n",
      "N1975177_1a_S226 78557.0\n",
      "N1975177_1b_S244 103220.0\n",
      "kept sampleID: N1975177_1b_S244\n",
      "\n",
      "Duplicated nida: 1975194.8\n",
      "N1975194_8a_S132 254361.0\n",
      "N1975194_8b_S242 132307.0\n",
      "kept sampleID: N1975194_8a_S132\n",
      "\n",
      "Duplicated nida: 1975247.1\n",
      "N1975247_1b_S186 267781.0\n",
      "N1975247_1a_S256 126918.0\n",
      "kept sampleID: N1975247_1b_S186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exploring the read counts for the duplicates and select the one with highest depth\n",
    "for nida in duplicated_nidas:\n",
    "    print(\"Duplicated nida:\", nida)\n",
    "    #Identify samples with the same nida\n",
    "    sample_list = massinga_data_merged.loc[massinga_data_merged['nida'] == nida, 'sampleID'].unique()\n",
    "    #Sum all read counts across all alleles for each sample\n",
    "    all_counts = np.zeros(len(sample_list))\n",
    "    for s, sample in enumerate(sample_list):\n",
    "        mask_sample = massinga_data_merged['sampleID'] == sample\n",
    "        all_counts[s] = massinga_data_merged.loc[mask_sample, ['sampleID', 'reads']].sum().iloc[1]\n",
    "        print(sample, all_counts[s])\n",
    "    #Identify the sample with highest depth\n",
    "    where_max_reads = np.where(all_counts == np.max(all_counts))[0][0]\n",
    "    kept_sample = sample_list[where_max_reads]\n",
    "    print(\"kept sampleID:\", kept_sample)\n",
    "    #filtering samples\n",
    "    for s, sample in enumerate(sample_list):\n",
    "        if sample != kept_sample:\n",
    "            massinga_data_merged = massinga_data_merged.loc[massinga_data_merged['sampleID'] != sample]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176ae202-864a-4e33-8112-e0ff21ce0b82",
   "metadata": {},
   "source": [
    "## Load REACT2 metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e3c4b7-c3ec-4443-afc1-96aba721daea",
   "metadata": {},
   "source": [
    "We proceed now with the data from Magude and Matutuine districts (Maputo province). We first load their metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "984e0f53-e830-44d1-9dc3-a514fc9a88c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "datapath = '/home/apujol/isglobal/projects/genmoz/data/react2/'\n",
    "react2_22_data = pd.read_csv(datapath + \"react2_index_nida_gps_data.csv\")\n",
    "\n",
    "#Keeping only cases with nida (not all entries have a nida)\n",
    "react22_all_nida = react2_22_data[react2_22_data['nida'].notnull()]\n",
    "react22_all_nida = react22_all_nida.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acb4f5d-2298-48d4-b83d-6c52687d4c5d",
   "metadata": {},
   "source": [
    "## Merge metadata with runs data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345194d2-909b-485b-bdc8-c4181a289364",
   "metadata": {},
   "source": [
    "We now merge the metadata to each run including samples from these districts from 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1feda2ec-20d6-4d10-858d-185dadc9670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "react2_merge1 = pd.merge(react2_run1_data, react22_all_nida, on = 'nida', how = 'left')\n",
    "react2_merge2 = pd.merge(react2_run2_data, react22_all_nida, on = 'nida', how = 'left')\n",
    "react2_merge3 = pd.merge(boh22_run_data, react22_all_nida, on = 'nida', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78f2b5d6-8256-444c-9273-653145683129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the third run (boh22_run_data) only these samples are of interest\n",
    "list_react2_nida_from_boh22 = [1974289.2, 1974249.6, 1941507.9, 1974170.3, \\\n",
    "                       1974006.5, 1974025.6, 1974016.4, 1941589.5, \\\n",
    "                       1941487.4, 1928588.7, 1974086.7, 1974532.9, \\\n",
    "                       1974063.8, 2038793.8, 2039543.8, \\\n",
    "                        1990098.8, 2084295.6, 2084332.8]\n",
    "\n",
    "#These nida didn't get successful libraries: 1974289.2, 1974170.3\n",
    "#We only select these samples from the run boh22_run\n",
    "mask = react2_merge3['nida'].notnull()&react2_merge3['nida'].isnull()\n",
    "for nida in list_react2_nida_from_boh22:\n",
    "    mask = mask | (react2_merge3['nida'] == nida)\n",
    "\n",
    "react2_merge3 = react2_merge3[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4066d5e-4feb-489d-a6e1-fa8882386d34",
   "metadata": {},
   "source": [
    "Now we merge all the three runs including allele data and metadata: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47fd7ef7-1a84-4f83-add1-2123e0563e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "react_data_merged = pd.concat([react2_merge1, react2_merge2, react2_merge3])\n",
    "#Specify source name\n",
    "react_data_merged['source'] = 'REACT2'\n",
    "#Setting province as Maputo in case of missing values\n",
    "react_data_merged.loc[react_data_merged['province'].isnull(), 'province'] = 'Maputo Provincia'\n",
    "#Only data with nida numbers are kept\n",
    "react_data_merged = react_data_merged[react_data_merged['nida'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0280f6-db7e-4042-b276-ab461e873c5c",
   "metadata": {},
   "source": [
    "We proceed to identify duplicates and keep the one with highest number of reads in each case: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "114ee137-3107-472b-bb99-349baa2ba7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated NIDA: [1941549.9 1941617.5 1974024.9 1929868.9 1929863.4 1941447.8 1941451.5\n",
      " 1941773.8 1941778.3 2038790.7 1941448.5]\n"
     ]
    }
   ],
   "source": [
    "#Identify duplicated nidas\n",
    "mask = react_data_merged[['allele', 'nida']].duplicated()\n",
    "duplicated_nidas = react_data_merged.loc[mask, 'nida'].unique()\n",
    "print(\"Duplicated NIDA:\", duplicated_nidas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d83d76b5-8a86-46ab-848e-2b7dc58eaf9f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated nida: 1941549.9\n",
      "N1941549_9b_S287_L001 304473.0\n",
      "N1941549_9a_S171_L001 310490.0\n",
      "kept sampleID: N1941549_9a_S171_L001\n",
      "\n",
      "Duplicated nida: 1941617.5\n",
      "N1941617_5a_S31_L001 652669.0\n",
      "N1941617_5b_S33_L001 752279.0\n",
      "kept sampleID: N1941617_5b_S33_L001\n",
      "\n",
      "Duplicated nida: 1974024.9\n",
      "N1974024_9b_S260_L001 125914.0\n",
      "N1974024_9a_S34_L001 492397.0\n",
      "kept sampleID: N1974024_9a_S34_L001\n",
      "\n",
      "Duplicated nida: 1929868.9\n",
      "N1929868_9_S47_L001 205416.0\n",
      "N1929868_9_S141 121567.0\n",
      "kept sampleID: N1929868_9_S47_L001\n",
      "\n",
      "Duplicated nida: 1929863.4\n",
      "N1929863_4_S127_L001 570452.0\n",
      "N1929863_4_S34 335180.0\n",
      "kept sampleID: N1929863_4_S127_L001\n",
      "\n",
      "Duplicated nida: 1941447.8\n",
      "N1941447_8b_S238 313984.0\n",
      "N1941447_8a_S121 107546.0\n",
      "kept sampleID: N1941447_8b_S238\n",
      "\n",
      "Duplicated nida: 1941451.5\n",
      "N1941451_5_S230_L001 391543.0\n",
      "N1941451_5_S66 340879.0\n",
      "kept sampleID: N1941451_5_S230_L001\n",
      "\n",
      "Duplicated nida: 1941773.8\n",
      "N1941773_8_S188_L001 579656.0\n",
      "N1941773_8_S289 476132.0\n",
      "kept sampleID: N1941773_8_S188_L001\n",
      "\n",
      "Duplicated nida: 1941778.3\n",
      "N1941778_3a_S186 67994.0\n",
      "N1941778_3b_S281 197406.0\n",
      "kept sampleID: N1941778_3b_S281\n",
      "\n",
      "Duplicated nida: 2038790.7\n",
      "N2038790_7_S249_L001 31047.0\n",
      "N2038790_7_S161 53339.0\n",
      "kept sampleID: N2038790_7_S161\n",
      "\n",
      "Duplicated nida: 1941448.5\n",
      "N1941448_5_S76_L001 623449.0\n",
      "N1941448_5_S194 273905.0\n",
      "kept sampleID: N1941448_5_S76_L001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exploring the read counts for the duplicates and select the one with highest depth\n",
    "for nida in duplicated_nidas:\n",
    "    print(\"Duplicated nida:\", nida)\n",
    "    #Identify samples with the same nida\n",
    "    sample_list = react_data_merged.loc[react_data_merged['nida'] == nida, 'sampleID'].unique()\n",
    "    #Sum all read counts across all alleles for each sample\n",
    "    all_counts = np.zeros(len(sample_list))\n",
    "    for s, sample in enumerate(sample_list):\n",
    "        mask_sample = react_data_merged['sampleID'] == sample\n",
    "        all_counts[s] = react_data_merged.loc[mask_sample, ['sampleID', 'reads']].sum().iloc[1]\n",
    "        print(sample, all_counts[s])\n",
    "    #Identify the sample with highest depth\n",
    "    where_max_reads = np.where(all_counts == np.max(all_counts))[0][0]\n",
    "    kept_sample = sample_list[where_max_reads]\n",
    "    print(\"kept sampleID:\", kept_sample)\n",
    "    #filtering samples\n",
    "    for s, sample in enumerate(sample_list):\n",
    "        if sample != kept_sample:\n",
    "            react_data_merged = react_data_merged.loc[react_data_merged['sampleID'] != sample]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44f2560-1155-4025-842f-74aab2bfc8a8",
   "metadata": {},
   "source": [
    "## Merging all samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869cb15b-e630-4ab5-90d9-8840d0456180",
   "metadata": {},
   "source": [
    "We are ready to merge all sample data from all data sources: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47d1643a-e692-4c52-b825-97dc1a43dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_merged = pd.concat([hfs_data_merged, massinga_data_merged, react_data_merged])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfd71df-e246-4f82-af22-5f4b906ff524",
   "metadata": {},
   "source": [
    "## Filter diversity loci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34bdeca-1230-403f-ad3b-516aba447fbf",
   "metadata": {},
   "source": [
    "In our analysis we will only consider the loci that are informative of diversity, so we will only keep those: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f05d479-79f6-45eb-83e6-eb4d13b1890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = all_data_merged['Category'] == 'Diversity'\n",
    "all_data_merged_div = all_data_merged[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a894f620-e1b8-4ac7-b3aa-b9769716129e",
   "metadata": {},
   "source": [
    "## Renaming alleles to be consistent between different runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e45cd1-14ea-4961-9a65-05a4febd1937",
   "metadata": {},
   "source": [
    "Allele names are assigned at each run independently. For this reason, the allele names do not necessarily correspond to the same sequence between different runs. In this script we unify all the allele names, so that each allele is named based on their specific strain (set in the variable pseudo_cigar) for all the complete set of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "323652f6-b660-4e6c-8a1c-43dfa9bb46a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_95200/4261091168.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data_merged_div['new_allele'] = all_data_merged_div['locus'] + ':' + all_data_merged_div['pseudo_cigar']\n"
     ]
    }
   ],
   "source": [
    "all_data_merged_div['new_allele'] = all_data_merged_div['locus'] + ':' + all_data_merged_div['pseudo_cigar']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b625f4ce-bc14-49a8-89fc-535dc3f9801e",
   "metadata": {},
   "source": [
    "## Apply coverage filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8f7a15-b805-4dab-97e2-f180eb5d8150",
   "metadata": {},
   "source": [
    "Filter 1: Alleles at diversity loci with <1% within-sample frequency are removed due to suspicion of faulty amplification or sequencing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5695ff82-4ba2-4d87-9ec4-3f599857f380",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_low_within_freq = all_data_merged_div['norm.reads.locus'] >= .01\n",
    "all_data_merged_div_filtMAF = all_data_merged_div[mask_low_within_freq]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bd1009-1743-4cbe-85e3-46fef2bdaea1",
   "metadata": {},
   "source": [
    "Filter 2: We only keep loci that have >=100 samples with >= 100 reads in that locus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50f0d1c2-6432-40e1-83a5-f15d3c31ed0b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loop over all loci\n",
    "for locus in all_data_merged_div['locus'].unique():\n",
    "    #Mask data from the locus\n",
    "    mask_locus = all_data_merged_div_filtMAF['locus'] == locus\n",
    "    #Count reads in locus per sample\n",
    "    reads_per_sample = all_data_merged_div_filtMAF.loc[mask_locus, ['sampleID', 'reads']].groupby('sampleID').sum()\n",
    "    #Count how many samples have >=100 reads\n",
    "    sample_100reads = reads_per_sample >= 100\n",
    "    total_samples_high_cov = sample_100reads.sum()\n",
    "    #Remove a locus if < 100 samples have >=100 reads\n",
    "    if total_samples_high_cov.iloc[0] < 100:\n",
    "        print(locus, total_samples_high_cov.iloc[0])\n",
    "        print(\"this locus is being filtered from all_data_merged_div\")\n",
    "        all_data_merged_div_filtMAF = all_data_merged_div_filtMAF[np.invert(mask_locus)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddebae3-720b-41c0-8559-24b2ec11dc72",
   "metadata": {},
   "source": [
    "No loci with < 100 reads, so filtering no needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1101873f-2ca5-405a-8b1a-0b80d201649d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of loci: 165\n"
     ]
    }
   ],
   "source": [
    "total_loci = all_data_merged_div_filtMAF['locus'].unique().shape[0]\n",
    "print(\"Total number of loci:\",total_loci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce121555-c866-4b8a-ac8c-5b5e4a59bb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 1666\n"
     ]
    }
   ],
   "source": [
    "all_samples = all_data_merged_div_filtMAF['sampleID'].unique()\n",
    "print(\"Total number of samples:\", all_samples.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c3a812-cb6f-43eb-bdda-fa84f83af439",
   "metadata": {},
   "source": [
    "Filter 3: Removing samples with less than 50 loci covered with at least 100 reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a63feb78-2017-41bd-a4c6-206031844229",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample N1923215_7_S29 with only 13 loci covered, removed ( HFS SMC SMC201 SMC201 )\n",
      "Sample N1938669_S72 with only 45 loci covered, removed ( HFS GenMoz HFS2201 HFS2201 )\n",
      "Sample N1941916_9_S102 with only 40 loci covered, removed ( HFS GenMoz HFS2201 HFS2201 )\n",
      "Sample N1958854_4_S222 with only 0 loci covered, removed ( HFS GenMoz HFS2201 HFS2201 )\n",
      "Sample N1960791_7_S170 with only 0 loci covered, removed ( HFS GenMoz HFS2201 HFS2201 )\n",
      "Sample N1960868_6_S261 with only 4 loci covered, removed ( HFS GenMoz HFS2201 HFS2201 )\n",
      "Sample N1934157_6_S241 with only 22 loci covered, removed ( HFS GenMoz HFS2201 HFS2201 )\n",
      "Sample N1933462_2_S49 with only 44 loci covered, removed ( HFS GenMoz HFS2203 HFS2203 )\n",
      "Sample N1960302_5_S28 with only 38 loci covered, removed ( HFS GenMoz HFS2203 HFS2203 )\n",
      "Sample N1961159_4_S11 with only 27 loci covered, removed ( HFS GenMoz HFS2203 HFS2203 )\n",
      "Sample N1963582_8_S3 with only 35 loci covered, removed ( HFS GenMoz HFS2203 HFS2203 )\n",
      "Sample N1934116_3_S100 with only 10 loci covered, removed ( HFS GenMoz HFS2203 HFS2203 )\n",
      "Sample N1936186_4_S64 with only 36 loci covered, removed ( HFS GenMoz HFS2203 HFS2203 )\n",
      "Sample N1963286_5_S59 with only 9 loci covered, removed ( HFS GenMoz HFS2203 HFS2203 )\n",
      "Sample N1935440_8_S23 with only 14 loci covered, removed ( HFS GenMoz HFS2203 HFS2203 )\n",
      "Sample N1942140_7_S89 with only 0 loci covered, removed ( HFS GenMoz HFS2203 HFS2203 )\n",
      "Sample N1942396_8_S62 with only 6 loci covered, removed ( HFS GenMoz HFS2203 HFS2203 )\n",
      "Sample N1933404_2_S16 with only 28 loci covered, removed ( HFS GenMoz HFS2203 HFS2203 )\n",
      "Sample N1959179_7_S61 with only 0 loci covered, removed ( HFS GenMoz HFS2203 HFS2203 )\n",
      "Sample N1922932_4_S103 with only 34 loci covered, removed ( HFS SMC SMC204 SMC204 )\n",
      "Sample N1923120_4_S151 with only 2 loci covered, removed ( HFS SMC SMC204 SMC204 )\n",
      "Sample N1818015_2_S49 with only 5 loci covered, removed ( HFS SMC SMC204 SMC204 )\n",
      "Sample N1948635_2_S30 with only 7 loci covered, removed ( HFS SMC SMC204 SMC204 )\n",
      "Sample N1948911_7_S140 with only 0 loci covered, removed ( HFS SMC SMC204 SMC204 )\n",
      "Sample N1967719_4_S24 with only 5 loci covered, removed ( HFS TES TES2201 TES2201 )\n",
      "Sample N1967785_9_S16 with only 21 loci covered, removed ( HFS TES TES2201 TES2201 )\n",
      "Sample N1970423_4_S280 with only 31 loci covered, removed ( HFS TES TES2201 TES2201 )\n",
      "Sample N1971080_8_S32 with only 0 loci covered, removed ( HFS TES TES2201 TES2201 )\n",
      "Sample N1970319_S277 with only 22 loci covered, removed ( HFS TES TES2201 TES2201 )\n",
      "Sample N1975253_2_S257 with only 11 loci covered, removed ( Multiply MULB MULT nan )\n",
      "Sample N1975525_S198 with only 0 loci covered, removed ( Multiply MULB MULT nan )\n",
      "Sample N1975552_6_S170 with only 38 loci covered, removed ( Multiply MULB MULT nan )\n",
      "Sample N1975585_4_S216 with only 16 loci covered, removed ( Multiply MULB MULT nan )\n",
      "Sample N1975613_4_S266 with only 9 loci covered, removed ( Multiply MULB MULT nan )\n",
      "Sample N1975456_7_S49 with only 0 loci covered, removed ( Multiply MULB MULT nan )\n",
      "Sample N1975504_5_S163 with only 12 loci covered, removed ( Multiply MULB MULT nan )\n",
      "Sample N1975534_2_S254 with only 1 loci covered, removed ( Multiply MULB MULT nan )\n",
      "Sample N1975150_4_S221 with only 0 loci covered, removed ( Multiply MULB MULT nan )\n",
      "Sample N1975236_5_S235 with only 21 loci covered, removed ( Multiply MULB MULT nan )\n",
      "Sample N1975280_8_S296 with only 13 loci covered, removed ( Multiply MULB MULT nan )\n",
      "Sample N1975274_7_S309 with only 6 loci covered, removed ( Multiply MULB MULT nan )\n",
      "Sample N1975175_7_S62 with only 0 loci covered, removed ( Multiply MULB MULT nan )\n",
      "Sample N1975423_9_S318 with only 3 loci covered, removed ( Multiply MULB MULT nan )\n",
      "Sample N1941361_7_S99_L001 with only 32 loci covered, removed ( REACT2 nan REACT_R1 nan )\n",
      "Sample N1929832_S267_L001 with only 4 loci covered, removed ( REACT2 nan REACT_R1 nan )\n",
      "Sample N1929976_1_S195_L001 with only 0 loci covered, removed ( REACT2 nan REACT_R1 nan )\n",
      "Sample N1974154_3_S226_L001 with only 0 loci covered, removed ( REACT2 nan REACT_R1 nan )\n",
      "Sample N2084422_6_S266 with only 39 loci covered, removed ( REACT2 nan REACT_R2 nan )\n",
      "Sample N1928743_S283 with only 5 loci covered, removed ( REACT2 nan REACT_R2 nan )\n",
      "Sample N1929877_1_S37 with only 41 loci covered, removed ( REACT2 nan REACT_R2 nan )\n",
      "Sample N1941510_9_S60 with only 14 loci covered, removed ( REACT2 nan REACT_R2 nan )\n",
      "Sample N2039551_3_S288 with only 11 loci covered, removed ( REACT2 nan REACT_R2 nan )\n",
      "Sample N1930002_3_S77 with only 19 loci covered, removed ( REACT2 nan REACT_R2 nan )\n",
      "Sample N2039497_4_S163 with only 6 loci covered, removed ( REACT2 nan REACT_R2 nan )\n",
      "Sample N2039550_6_S265 with only 48 loci covered, removed ( REACT2 nan REACT_R2 nan )\n",
      "Sample N2084426_4_S269 with only 34 loci covered, removed ( REACT2 nan REACT_R2 nan )\n",
      "Sample N2084517_9_S331 with only 2 loci covered, removed ( REACT2 nan REACT_R2 nan )\n",
      "Sample N1941779_S120 with only 0 loci covered, removed ( REACT2 nan REACT_R2 nan )\n",
      "Sample N1974236_6_S255 with only 0 loci covered, removed ( REACT2 nan REACT_R2 nan )\n",
      "Sample N2084267_3_S279 with only 4 loci covered, removed ( REACT2 nan REACT_R2 nan )\n",
      "Sample N1929826_9_S228 with only 0 loci covered, removed ( REACT2 nan REACT_R2 nan )\n"
     ]
    }
   ],
   "source": [
    "min_loci = 50\n",
    "all_data_div_covered_samples = pd.DataFrame(all_data_merged_div_filtMAF)\n",
    "for sample in all_samples:\n",
    "    #mask to select sample\n",
    "    mask_sample = all_data_div_covered_samples['sampleID'] == sample\n",
    "    #count reads per locus\n",
    "    locus_reads = all_data_div_covered_samples.loc[mask_sample, ['locus', 'reads']].groupby('locus').sum()\n",
    "    #count loci with >= 100 reads\n",
    "    n_good_loci = np.sum(locus_reads['reads'] >= 100)\n",
    "    if n_good_loci < min_loci:\n",
    "        mask_sample = all_data_div_covered_samples['sampleID'] == sample\n",
    "        #for printing on screen\n",
    "        run_id = all_data_div_covered_samples.loc[mask_sample, 'run_id_resmark'].unique()[0]\n",
    "        run = all_data_div_covered_samples.loc[mask_sample, 'run'].unique()[0]\n",
    "        study = all_data_div_covered_samples.loc[mask_sample, 'study'].unique()[0]\n",
    "        source = all_data_div_covered_samples.loc[mask_sample, 'source'].unique()[0]\n",
    "        print(\"Sample \" + sample + \" with only\", n_good_loci, \"loci covered, removed (\", source, study, run, run_id, \")\")\n",
    "        #removing sample\n",
    "        all_data_div_covered_samples = all_data_div_covered_samples[np.invert(mask_sample)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e0b71c-e6be-4124-9231-500d15c541dc",
   "metadata": {},
   "source": [
    "Filter 4: When replicate samples, keep the one with the highest read count (depth). In this case we have no replicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "873ff4fa-05ce-4287-b4c7-d110f21d80b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check that no replicates present\n",
    "all_data_div_covered_samples[['sampleID', 'allele']].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5736666-4e3e-4ae9-895b-c59ab038e716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check that no replicates present\n",
    "all_data_div_covered_samples[['nida', 'allele']].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dbc93d11-87bc-4cba-adf2-bfe67321aa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of samples: 1605\n"
     ]
    }
   ],
   "source": [
    "all_samples_filtered = all_data_div_covered_samples['sampleID'].unique()\n",
    "print(\"Final number of samples:\", len(all_samples_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577bb3c5-1dae-4625-98ca-49379fb49201",
   "metadata": {},
   "source": [
    "## Save filtered samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c931b8f-d1f6-4373-96b0-24c9fa180030",
   "metadata": {},
   "source": [
    "We save the final data that will be used for the whole analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f9d6e0f5-f95c-43a9-971f-80cddb043202",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/home/apujol/isglobal/manuscripts/importation_relatedness/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "919cdfca-60b4-4f23-bbbd-8968bc31b070",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_div_covered_samples.to_csv(save_path + 'data_filtered_for_study.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819335cb-a2a5-4544-8e5a-9ddc2891eede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
